save_path: /root/computer/computer/train_dataset_encoded6/sb_diffusion_freezernn_contfiltered_unfreeze_afterchallenging_newdata_pretrainchallenging_addc_allnew_more_c_alldata_diffusion_c_allc_unfreeze

model:
  base_learning_rate: 8e-05
  target: latent_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    pretrain: false
    freezernn: false
    run_eval: false
    pretrain2: false
    pretrain3: false
    scheduled_sampling_rate: 0.0
    scheduled_sampling_length: 1
    scheduled_sampling_ddim_steps: 8
    linear_start: 0.0015
    linear_end: 0.0195
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: image
    cond_stage_key: action_
    hybrid_key: c_concat
    image_size: [64, 48]
    channels: 3
    cond_stage_trainable: false
    conditioning_key: hybrid
    monitor: val/loss_simple_ema

    unet_config:
      target: latent_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: [64, 48]
        in_channels: 48
        out_channels: 16
        model_channels: 512
        attention_resolutions: []
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        num_head_channels: 32
        use_spatial_transformer: false
        transformer_depth: 1

    temporal_encoder_config:
      target: latent_diffusion.ldm.modules.encoders.temporal_encoder.TemporalEncoder
      params:
        input_channels: 16
        hidden_size: 4096
        num_layers: 1
        dropout: 0.1
        output_channels: 32
        output_height: 48
        output_width: 64

    first_stage_config:
      target: latent_diffusion.ldm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 16
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 16
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity

    cond_stage_config: __is_unconditional__
    scheduler_config:
      target: latent_diffusion.ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps: [1000]          # adjust your warmup steps
        cycle_lengths: [1000000000]    # essentially "infinite" if no decay
        f_start: [1.e-6]               # initial LR multiplier
        f_max: [1.]                    # max LR multiplier after warmup
        f_min: [1.]                    # keep 1.0 for constant LR post-warmup

data:
  target: data.data_processing.datasets.DataModule
  params:
    batch_size: 8
    num_workers: 4
    wrap: false
    shuffle: True
    drop_last: True
    pin_memory: True
    prefetch_factor: 2
    persistent_workers: True
    train:
      target: data.data_processing.datasets.ActionsData
      params:
        debug_mode: false
        data_csv_paths:
          - train_dataset_encoded/filtered_dataset.target_frames.train.challenging.csv
          - train_dataset_encoded2/train_dataset_apr3_encoded/filtered_dataset.target_frames.challenging.csv
          - train_dataset_encoded3/train_dataset_apr2_encoded/filtered_dataset.target_frames.challenging.csv
          - train_dataset_encoded4/train_dataset_apr5_encoded/filtered_dataset.target_frames.challenging.csv
          - train_dataset_encoded5/train_dataset_apr14_2_encoded/filtered_dataset.target_frames.challenging.csv
          - train_dataset_encoded6/train_dataset_apr14_3_encoded/filtered_dataset.target_frames.challenging.csv
        normalization: standard
        context_length: 32

lightning:
  trainer:
    benchmark: True
    max_epochs: 64000
    limit_val_batches: 0
    accelerator: gpu
    devices: 8
    strategy: ddp_find_unused_parameters_false
    accumulate_grad_batches: 1
    gradient_clip_val: 1
