save_path: ./sb_computecanada_fsdp_noema_challengingandsample_pretrainrnn_balanced_lr5e6_contbest_samplercover_newd_contfreezernn_newnewd_origunet_nospatial2_ONLINE_online_online_x0_ddpm32

model:
  base_learning_rate: 5e-06
  target: latent_diffusion.ldm.models.diffusion.ddpm.LatentDiffusion
  params:
    parameterization: x0
    use_ema: false
    pretrain: false
    freezernn: True
    run_eval: false
    pretrain2: false
    pretrain3: false
    scheduled_sampling_rate: 0.00
    scheduled_sampling_length: 1
    scheduled_sampling_ddim_steps: 32
    linear_start: 0.0015
    linear_end: 0.0195
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 32
    first_stage_key: image
    cond_stage_key: action_
    hybrid_key: c_concat
    image_size: [64, 48]
    channels: 3
    cond_stage_trainable: false
    conditioning_key: hybrid
    monitor: val/loss_simple_ema

    unet_config:
      target: latent_diffusion.ldm.modules.diffusionmodules.openaimodel.UNetModel
      params:
        image_size: [64, 48]
        in_channels: 48
        out_channels: 16
        model_channels: 192
        attention_resolutions:
        - 8
        - 4
        - 2
        num_res_blocks: 2
        channel_mult:
        - 1
        - 2
        - 3
        - 5
        num_head_channels: 32
        use_spatial_transformer: false
        transformer_depth: 1

    temporal_encoder_config:
      target: latent_diffusion.ldm.modules.encoders.temporal_encoder.TemporalEncoder
      params:
        input_channels: 16
        hidden_size: 4096
        num_layers: 1
        dropout: 0.1
        output_channels: 32
        output_height: 48
        output_width: 64

    first_stage_config:
      target: latent_diffusion.ldm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 16
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 16
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
          - 1
          - 2
          - 4
          - 4
          num_res_blocks: 2
          attn_resolutions: []
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity

    cond_stage_config: __is_unconditional__
    scheduler_config:
      target: latent_diffusion.ldm.lr_scheduler.LambdaLinearScheduler
      params:
        warm_up_steps: [1000]          # adjust your warmup steps
        cycle_lengths: [1000000000]    # essentially "infinite" if no decay
        f_start: [1.e-6]               # initial LR multiplier
        f_max: [1.]                    # max LR multiplier after warmup
        f_min: [1.]                    # keep 1.0 for constant LR post-warmup

data:
  target: data.data_processing.datasets.DataModule
  params:
    batch_size: 16
    num_workers: 4
    wrap: false
    shuffle: True
    drop_last: True
    pin_memory: True
    prefetch_factor: 1
    use_balanced_sampling: True
    persistent_workers: True
    train:
      target: data.data_processing.datasets.ActionsData
      params:
        debug_mode: false
        data_csv_paths:
          - ../autoencoder/train_dataset_may20_9_webdataset_encoded/filtered_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_9_webdataset_encoded/train_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_10_webdataset_encoded/filtered_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_10_webdataset_encoded/train_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_11_webdataset_encoded/filtered_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_11_webdataset_encoded/train_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_12_webdataset_encoded/filtered_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_12_webdataset_encoded/train_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_13_webdataset_encoded/filtered_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_13_webdataset_encoded/train_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_14_webdataset_encoded/filtered_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_14_webdataset_encoded/train_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_15_webdataset_encoded/filtered_dataset.target_frames.csv
          - ../autoencoder/train_dataset_may20_15_webdataset_encoded/train_dataset.target_frames.csv
          - ../../neuralos-demo/train_dataset_encoded_online/train_dataset.target_frames.csv
        normalization: standard
        context_length: 64

lightning:
  trainer:
    reload_dataloaders_every_n_epochs: 1
    benchmark: True
    max_epochs: 64000
    limit_val_batches: 0
    accelerator: gpu
    devices: 8
    strategy: fsdp
    accumulate_grad_batches: 1
    gradient_clip_val: 1
